<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dan</title>
    <description></description>
    <link>http://dannyhnu.github.io/</link>
    <atom:link href="http://dannyhnu.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 26 Nov 2018 16:02:37 +0800</pubDate>
    <lastBuildDate>Mon, 26 Nov 2018 16:02:37 +0800</lastBuildDate>
    <generator>Jekyll v3.1.3</generator>
    
      <item>
        <title>运营-监控方法笔记</title>
        <description>&lt;p&gt;我们的业务是新闻APP的广告数据，主要监控的场景是广告的pv、click和收入。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;监控需求&lt;/h2&gt;

&lt;h3&gt;实时性&lt;/h3&gt;

&lt;p&gt;从实时性上来区分，我们的监控场景主要可以分为天级（T+1）、小时级（H+1）、分钟级(M+1)。其中天级监控的场景主要是收入相关指标，比如ecpm、cpc等。这类曲线一般波动较小，且业务特性比较强。 而小时、分钟级监控的场景为广告实时流量，这类曲线波动大且不规律。&lt;/p&gt;

&lt;h3&gt;粒度&lt;/h3&gt;

&lt;p&gt;因为广告业务配置多而繁杂，单个维度的变化都能引起广告收入的波动。所以我们对实时流量做了全维度监控。&lt;/p&gt;

&lt;h2&gt;监控策略&lt;/h2&gt;

&lt;p&gt;Shark采用的监控策略可以分为常规方法、时间序列基线、相似波几种。
* 组合策略: 一般对于波动比较大的监控场景，我们会采用几个策略的组合来判定是否异常。比如五个策略有三个判定异常则认定异常。&lt;br&gt;
* 触发告警: 为了减少误告和防止告警轰炸，我们设定了触发告警的若干条件，比如次数、频率限制。&lt;/p&gt;

&lt;h3&gt;常规监控&lt;/h3&gt;

&lt;p&gt;对于广告收入指标或者T+1时间序列的监控(这类指标数据一般 波动较小 ，而且表现出 强业务特性 )，我们多采用比较常规的监控策略。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;同环比&lt;/p&gt;

&lt;p&gt;  环比 : f(t)/f(t-1)&lt;br&gt;
  同比 : f(t)/f(t-c)。c为周期，一般为三分钟、7天、30天等。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;动态阈值&lt;/p&gt;

&lt;p&gt;  历史极值 ：取历史指定若干周期的数据的极值，差值超过 三倍方差 则判定异常。&lt;br&gt;
  移动平均 ：取历史指定若干周期的数据，去除极大、小值(消除历史异常点的影响)取平均。差值超过 三倍方差 则判定异常。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EWMA
  指数加权移动平均 ：取历史指定若干周期的数据，去除极大、小值(消除历史异常点的影响)：&lt;br&gt;f(t)=αf(t−1)+(1−α)θt&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;时间序列基线&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;时间序列监控主要面向的业务场景是 分钟级 的流量监控（ 波动大，而且可能极不规律 ）。基于时间序列预测的基本想法是：我们用监控维度的过去 若干个周期 的时间序列去预测下一个周期，用 预测 出来的时间序列作为基线，然后拿当天的时间序列和基线进行 比较 ，超过一定范围则触发告警。解释下：
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;分钟级: 我们对流量数据按分钟切片，如果有业务波动可以在分钟内发现。&lt;/li&gt;
&lt;li&gt;若干个周期: 我们忽略了季节性等因素，只考虑了一周之内的波动。所以选取了7|14天为历史周期。&lt;/li&gt;
&lt;li&gt;预测: 预测方法用的是EWMA和Holt winters。&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;时间序列的自相似性&lt;/h4&gt;

&lt;p&gt;用时间序列预估的方法来做分钟级的曲线预估，需要曲线满足一定的自相似性。当然幸好我们的业务在很多维度上都表现出了明显的周期性、稳定性。如果和股市一样飘忽不定的话，那预估就是扯淡了。先声明下这里说的相似性和时间序列的平稳性不是一个概念。计算时间序列的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38130622&quot;&gt;距离&lt;/a&gt;有很多方法。这里我们采用欧式空间的距离：
    对于时间序列x={x1,x2…}, y={y1,y2…}， d(x, y) = Σ(abs(xi - yi)/avg(x))
以新闻广告的位置类型为例，不同位置在相邻周期的距离差别很大：&lt;br&gt;
&lt;img src=&quot;/images/ops/dis.PNG&quot; alt=&quot;note&quot; /&gt;&lt;br&gt;
    我们采用若干周期内 距离的标准差 来度量两个时间序列的 自相似性: &lt;strong&gt;self_sim = sqrt((xi - mean(ti)&lt;sup&gt;2&lt;/sup&gt;)/freq)&lt;/strong&gt;&lt;/p&gt;

&lt;h4&gt;自相似时间序列监控&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;对于相邻周期内相似的时间序列(周期内的波动比较稳定)，通过一般的时间预测方法都能预测出比较好的时间基线。以新闻广告的列表信息流为例，基线和实际序列的差值可以稳定在一定的范围内：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/ops/ts_1.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;非自相似时间序列监控&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;对于相邻周期内不相似的时间序列，比较直接的想法是 转换成自相似时间序列 。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;拉宽时间窗口
以新闻插件广告位为例，由于新闻push会带来流量的飙升，而每天push的时间点又不一样，所以流量的峰值在每天都不太相同。所以分钟级的基线预测不太理想： &lt;img src=&quot;/images/ops/ts_2.png&quot; alt=&quot;note&quot; /&gt;
如果将时间窗口拉大，比如以两个小时为例，则可以减小预测误差：
&lt;img src=&quot;/images/ops/ts_3.png&quot; alt=&quot;note&quot; /&gt;
拉宽时间窗口其实是 牺牲了时效性来平滑时间序列 ，所以通常不太推荐这个方法。&lt;/li&gt;
&lt;li&gt;差分时间序列
这里说的差分包括并不限于对时间序列作n阶差分、转换为增长率(同比或者环比)序列。
以新闻banner广告为例，由于每天的客户数波动较大，所以广告曲线的振幅也不一样。
&lt;img src=&quot;/images/ops/ts_4.png&quot; alt=&quot;note&quot; /&gt;
但是每天曲线的升降趋势却相似，所以转换成变化速度曲线：f`(t) =( f(t)- f(t-n) )/ f(t-n), n为时间跨度。
&lt;img src=&quot;/images/ops/ts_5.png&quot; alt=&quot;note&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;相似波&lt;/h3&gt;

&lt;p&gt;由于广告业务的多样性和媒体的推送操作的不确定性(新闻每天不定时的push)，会导致很多维度的广告PV曲线并没有自相似性甚至毫无规律。
 &lt;img src=&quot;/images/ops/ts_6.png&quot; alt=&quot;note&quot; /&gt;
对于这类曲线，我们监控的主要目标是 波动特性是否在合理的范围 内。所以我们抽取出曲线的波，通过和历史的正常波形进行比较来判断是否异常。&lt;/p&gt;

&lt;h4&gt;波的度量&lt;/h4&gt;

&lt;p&gt; 我们认为一个波的生命周期从上升率大于某个特定值为波的 上升期 ，到达 波峰 后变化率变为负进入 下降期 ，当下降率恢复到某个特定值则为到达 波谷 。在一个波的完整生命周期内我们会抽取出这个波的一些度量：&lt;br /&gt;
* 上升率 ： 波在上升期的增长率&lt;br /&gt;
* 波峰 ： 波的最高值&lt;br /&gt;
* 上半生波长 ：上升期的波长&lt;br /&gt;
* 下半生波长 ：下降期的波长&lt;br /&gt;
* 下降率 ：波在下降期的下降率&lt;br /&gt;
* 波谷 ：波的最低值&lt;br /&gt;
 为了平滑小区间内的波动，我们滑动一个固定宽度的时间窗口。把一个窗口内的点 拟合成一条直线 。&lt;/p&gt;

&lt;h4&gt;波的监控&lt;/h4&gt;

&lt;p&gt; 在波不同的生命周期内我们会触发不同的监控策略，最后组合多个策略判断波动是否异常：&lt;br /&gt;
* 上升率 ：上升期内上升率如果超过历史正常上升率的极大极小值，则标记异常。&lt;br /&gt;
* 波峰 ： 波峰和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 上半生波长 ：上半生波长和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 下半生波长 ：下半生波长和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 下降率 ：下降期内下降率如果超过历史正常下降率的极大极小值，则标记异常。&lt;br /&gt;
* 波谷 ：波谷和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Nov 2018 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2018/11/20/shark-monitor/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2018/11/20/shark-monitor/</guid>
        
        
        <category>ops</category>
        
      </item>
    
      <item>
        <title>关于大数据查询系统的一些思考</title>
        <description>&lt;p&gt;大数据计算框架日新月异，而每次计算效率的大幅提升，简单总结就一句话：IO优化。不管是存储，还是计算的优化，都是更少的磁盘IO，更多的内存IO。本文结合本人在自身工作中的经验，主要讲一下对大数据计算效率上的一些思考。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;技术选型&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;对大数据查询平台的技术选型，可以从存储和计算两个方面结合自身业务来衡量。
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;存储&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;扫表vs 索引：建索引可以快速locate需要查询的数据，完全省去了对查询不相关数据的读写IO。但是代价却是漫长的建索引过程和大量额外的存储开销。所以建索引适合经常带filter的数据筛选计算和数据维度基数较多的业务场景。对于一些不带filter的查询和大范围时间内的聚合计算业务场景，建索引就显得多余了&lt;/li&gt;
&lt;li&gt;列存vs 行存：相比于传统DB的行存，列存能大量减少磁盘IO。比如更高效的数据压缩、查询不需要解压非相关字段、更好的索引策略等。但是在数据更新场景下，列存会带来一些额外的开销。所以对更新频繁的业务场景，选择列存就要慎重了。&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;计算&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Map reduce vs MPP: 个人觉得二者的边界比较模糊。Map reduce架构扩展性强、可用性高。MPP由于一些不共享约束，使得计算效率相比而言更高(多内存IO，当然对硬件要求也更高)。所以个人觉得，对于一些离线、近实时处理Map reduce架构更可靠(如hive、spark)。而实时计算，多采用MPP架构（比如业界比较流行的计算框架GP、druid、clickhouse等）。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;数据预处理&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;对于数据计算过程最耗IO的两个步骤：JOIN和GROUP BY，可以在数据预处理阶段节省计算过程的IO。
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;预JOIN：大宽表设计，可以省掉计算过程的join IO，这个可能是比较常用的操作，就不多赘述。可能需要考虑的一点是对于经常变动的业务字段，不太适合预JOIN，因为重跑历史数据真的是非常头疼。所以预join的字段最好是不常变动的。&lt;/li&gt;
&lt;li&gt;数据CUBE：预先group by，简单来说就是空间换时间。具体可以参考apache kylin。但是在海量、高维度的业务场景下，野蛮预建cube的存储代价太大。所以我们提出一种基于组合优化的物化cube智能生成方法：
① 从L(n)出发，计算得到满足覆盖率t的所有n-1规模集合L(n-1), 进一步迭代 L(n-2), L(n-3), ...L(n-k), 删选出所有满足最低覆盖率t的组合C；&lt;br&gt;
② 计算每个组合Ci的空间占用Vi 和收益Ri[(r1/r2)*Coverage(pattern)]；&lt;br&gt;
③ 转化为0\1背包问题：在满足空间限制V的情况下，选出最大收益R的集合Q。&lt;br&gt;
简单来说，就是根据历史查询选出综合考虑查询覆盖和空间占用的最佳组合。下图是查询是否命中cube的时间对比。&lt;br&gt;
&lt;img src=&quot;/images/hadoop/cube.png&quot; alt=&quot;note&quot; /&gt;

&lt;h2&gt;查询计划优化&lt;/h2&gt;&lt;/li&gt;
&lt;li&gt;分区筛选：根据业务特点可对用户查询添加特定分区筛选条件。举一个我们业务场景的case，当用户查特定广告排期(非分区字段)，我们在查询计划加上了筛选该排期下订单(分区字段)的条件，这样省略了大量非必要分区的读写IO。&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 20 Jun 2018 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2018/06/20/shark/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2018/06/20/shark/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>spark  - 那些坑</title>
        <description>&lt;h3&gt;JobProgressListener问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;Spark Application跑一段时间总会被yarn kill掉，报的错是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Container ... is running beyond physical memory limits. Current usage: xxx GB of xxx GB physical memory used; xx GB of xx GB virtual memory used. Killing container.&lt;/code&gt;&lt;/pre&gt;


&lt;!-- more --&gt;


&lt;p&gt;第一反应怀疑哪里有内存泄露，所以dump内存看了下，发现&lt;strong&gt;org.apache.spark.ui.jobs.JobProgressListener&lt;/strong&gt;几乎占掉了一半的内存。于是看代码，发现&lt;em&gt;stageIdToData&lt;/em&gt;和&lt;em&gt;jobIdToData&lt;/em&gt;两个变量保存了所有的job信息和stage信息。好在有&lt;em&gt;trimStagesIfNecessary&lt;/em&gt;和&lt;em&gt;trimJobsIfNecessary&lt;/em&gt;两个方法，当job数或者stage数超过配置的数目，JobProgressListener会每次remove掉10%的job或者stage。&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;spark.ui.retainedJobs
spark.ui.retainedStages&lt;/code&gt;&lt;/pre&gt;


&lt;h3&gt;Broadcast问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;使用broadcast的时候发现task耗费大量时间在&lt;em&gt;Task Deserialization Time&lt;/em&gt;上，且当广播的变量越大task反序列化的时间也越大。于是看到&lt;a href=&quot;http://dongguo.me/blog/2014/12/30/Spark-Usage-Share/&quot;&gt;这篇文章&lt;/a&gt;中提到的&lt;em&gt;正确地使用广播变量(broadcast variables)&lt;/em&gt;时才发现自己在真正产生action的操作前就读取broadcast的变量，导致每次广播都会在driver收集变量然后分发给各个executor。这是问题代码（java）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Function&lt;Row, Row&gt; mapFunc = new Function&lt;Row, Row&gt;() {
    private Map&lt;String, Double&gt; joinMap = broadcastVal.getValue();
    @Override
    public Row call(Row row) throws Exception {
        joinMap.get....
    }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;在action方法内部调用&lt;em&gt;getValue()&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Function&lt;Row, Row&gt; mapFunc = new Function&lt;Row, Row&gt;() {
    @Override
    public Row call(Row row) throws Exception {
        Map&lt;String, Double&gt; joinMap = broadcastVal.getValue();
        joinMap.get....
    }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h3&gt;spark sql聚合函数问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;先说下背景。为了最大限度的减少每次查询cover的数据，我们的数据模型不仅按月分表，而且单月的数据都会做行转列的操作。简单点说就是把指标项转成31列，随着时间的发展去填充列而不是增加数据行数。这样做的好处就是每个月第一天和到最后一天的数据量都会差不多，而这样做的代价就是更加负责的sql和每次查询都要做列转行的反转。
举个栗子，查询20160301到20160325分年龄的收入，一般的sql是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT age, SUM(income) FROM  table_201603 WHERE date BETWEEN 20160301 AND 20160325 GROUP BY age&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;但是我们得这样处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT age, SUM(income_1)  AS inome_20160301, SUM(income_2)  AS inome_20160302... , SUM(income_25)  AS inome_20160325 FROM  table_201603 GROUP BY age&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;然后再flatten查询结果。
因为一般的查询结果集比较小，所以flatten结果集的代价也比较小。但是这样做在spark sql上碰到一个坑，当一条sql中&lt;strong&gt;SUM&lt;/strong&gt;的数量超过24个时候会严重影响sql的执行速度(5-10倍的性能差)。
//TODO 原因待查&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;目前的spark 1.6 还是存在这个问题，所以只好做和谐处理。当查询中&lt;strong&gt;SUM&lt;/strong&gt;超过24个，将sql拆成多条sql处理。&lt;/p&gt;

&lt;h3&gt;spark sql数据类型问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;数据中有个&lt;em&gt;曝光&lt;/em&gt;字段，生成模型的时候定义成了&lt;em&gt;int&lt;/em&gt;类型。sql的聚合函数&lt;em&gt;SUM&lt;/em&gt;会把结果转成&lt;em&gt;long&lt;/em&gt;类型。但是在测试的时候发现结果会变小。这个坑比较隐蔽，貌似只有当在单个task内的聚合值超过21亿的时候才会影响最终结果。&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;数据源把&lt;em&gt;int&lt;/em&gt;改成&lt;em&gt;long&lt;/em&gt;类型就OK了&lt;/p&gt;
</description>
        <pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2016/05/17/spark-watchouts/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2016/05/17/spark-watchouts/</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>spark core - Broadcast</title>
        <description>&lt;p&gt;最近刚完成一个spark sql项目：大数据实时查询引擎。所有的数据模型整合成大宽表以parquet(列存，高压缩比，完美整合spark sql)格式存在hdfs上。做成大宽表的好处是，一些费时的join操作在预处理阶段就做好了，查询时只会有简单的group by等操作。但是由于业务的特点，我们的历史数据会变(坑爹啊)，而且没有时限(一两年前的说变就变)。好在这部分变化的数据不大，可以经过&lt;strong&gt;spark broadcast&lt;/strong&gt;广播到各个executor进行rdd map join。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;说回这篇文章的主题-spark broadcast。spark目前有两种广播的方法：http和p2p。二者的区别是各个executor在第一次怎么拿到广播变量。一旦executor拿到广播变量后就会立即放到本地storage。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 把广播变量缓存到BlockManager，这样后续的任务就不需要再去重新fetch了
SparkEnv.get.blockManager.putSingle(blockId, value_, StorageLevel.MEMORY_AND_DISK, tellMaster = false)
&lt;/code&gt;&lt;/pre&gt;


&lt;h4&gt;HttpBroadcast&lt;/h4&gt;

&lt;p&gt;Http的广播方法比较简单，在driver写一个local file，起一个http server。所以executor在&lt;em&gt;getValue&lt;/em&gt;的时候其实就是通过url访问driver上的http server拿到广播变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//！代码都精简过的
/*
 * driver 写local file
 */
private def write(id: Long, value: Any) {
    // 拿到一个本地文件
    val file = getFile(id) 
    val fileOutputStream = new FileOutputStream(file)
    ...
   // 序列化
    val ser = SparkEnv.get.serializer.newInstance()
    val serOut = ser.serializeStream(out)
    ...
    serOut.writeObject(value)
 }
 /*
 * 起http server
 */
private def createServer(conf: SparkConf) {
  broadcastDir = Utils.createTempDir(Utils.getLocalDir(conf), &quot;broadcast&quot;)
  val broadcastPort = conf.getInt(&quot;spark.broadcast.port&quot;, 0)
  server =
    new HttpServer(conf, broadcastDir, securityManager, broadcastPort, &quot;HTTP broadcast server&quot;)
  server.start()
  serverUri = server.uri
}
 /*
 * 读广播变量
 */
private def read[T: ClassTag](id: Long): T = {
  // 组装请求
  val url = serverUri + &quot;/&quot; + BroadcastBlockId(id).name
  uc = new URL(url).openConnection()
  ...
   //请求广播变量
  val inputStream = uc.getInputStream
  val in = new BufferedInputStream(inputStream, bufferSize)
  ...
  // 反序列化
  val ser = SparkEnv.get.serializer.newInstance()
  val serIn = ser.deserializeStream(in)
}
&lt;/code&gt;&lt;/pre&gt;


&lt;h4&gt;TorrentBroadcast&lt;/h4&gt;

&lt;p&gt;Torrent采用spark storage模块广播数据。首先会按照特定大小（默认4M）对数据进行分块,  然后把各个piece写到storage。第一次读取变量也是从driver或者其他的executor拿到各个piece然后保存到本地。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//！代码都精简过的
/*
 * 数据写入storage
 */
private def writeBlocks(value: T): Int = {
  // driver上保存一份广播数据，防止重复存储
  SparkEnv.get.blockManager.putSingle(broadcastId, value, StorageLevel.MEMORY_AND_DISK,
    tellMaster = false)
   //对数据进行分块
  val blocks =
    TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec)
    // 各个块写到storage
  blocks.zipWithIndex.foreach { case (block, i) =&gt;
    SparkEnv.get.blockManager.putBytes(
      BroadcastBlockId(id, &quot;piece&quot; + i),
      block,
      StorageLevel.MEMORY_AND_DISK_SER,
      tellMaster = true)
  }
  blocks.length
}
 /*
 * 读取数据
 */
 private def readBlocks(): Array[ByteBuffer] = {
...
  //循环各个数据块
  for (pid &lt;- Random.shuffle(Seq.range(0, numBlocks))) {
    val pieceId = BroadcastBlockId(id, &quot;piece&quot; + pid)
    // 从本地取
    def getLocal: Option[ByteBuffer] = bm.getLocalBytes(pieceId)
    // 本地取不到则从远程取
    def getRemote: Option[ByteBuffer] = bm.getRemoteBytes(pieceId).map { block =&gt;
      // 取到保存到本地
      SparkEnv.get.blockManager.putBytes(
        pieceId,
        block,
        StorageLevel.MEMORY_AND_DISK_SER,
        tellMaster = true)
      block
    }
    val block: ByteBuffer = getLocal.orElse(getRemote).getOrElse(
      throw new SparkException(s&quot;Failed to get $pieceId of $broadcastId&quot;))
    blocks(pid) = block
  }
  blocks
}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Mon, 16 May 2016 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2016/05/16/spark-broadcast/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2016/05/16/spark-broadcast/</guid>
        
        
        <category>spark</category>
        
      </item>
    
      <item>
        <title>豆瓣读书推荐</title>
        <description>&lt;p&gt;之前还一直跟同事抱怨说做数据挖掘，没有数据挖个毛线啊。 于是naza很happy的把豆瓣读书的数据都爬下来了 ‘恩 你挖个够吧’。好吧， 一直想看看豆瓣读书的推荐到底是怎么做的。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;豆瓣读书的推荐主要有两个页面：
&lt;br /&gt;
一个是每本书的主页， 比如梁文道的&lt;a href=&quot;http://book.douban.com/subject/4031698/&quot;&gt;读者&lt;/a&gt;。豆瓣会有一个推荐的section: &lt;em&gt;喜欢读&quot;读者&quot;的人也喜欢...&lt;/em&gt;。 这句话似乎是说推荐是基于&lt;strong&gt;协同过滤&lt;/strong&gt;的。我很奇怪为什么豆瓣会用这样的标题，这里明显不会是基于协同的推荐， 因为我用不同的用户登录， 推荐的结果是一样的。从推荐的结果来也像是&lt;strong&gt;content based&lt;/strong&gt;， 更准确点是&lt;strong&gt;tag-based&lt;/strong&gt;。 因为推荐的大多还是梁文道的书，还有些书tag是&lt;em&gt;随笔&lt;/em&gt;、&lt;em&gt;文化&lt;/em&gt;什么的。
&lt;br /&gt;
另一个是&lt;a href=&quot;http://book.douban.com/recommended&quot;&gt;豆瓣猜&lt;/a&gt;。首先会列出你可能感兴趣的新书。之后是你可能感兴趣的tag和book。这里应该是混合推荐了。
&lt;br /&gt;
当然，这都是猜想。试试看吧。&lt;/p&gt;

&lt;h4&gt;Warm-up&lt;/h4&gt;

&lt;p&gt;先解释下数据模型，爬的数据有people，book，tag以及相互的对应关系。构建三个matrix就可以将三者联系起来了。如图:
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/douban_model.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
再说明下， 所有的算法都是based on mahout。&lt;/p&gt;

&lt;h4&gt;tag-based&lt;/h4&gt;

&lt;p&gt;给定一本书， 找到一本tag相似的书。可以把book-tag matrix作input，这样tag就相当于book的dimension, 可以用&lt;em&gt;PearsonCorrelationSimilarity&lt;/em&gt;， 即皮尔逊系数来衡量book之间的距离。这样找相似的书， 即找最近的邻居：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
NearestNUserNeighborhood neighbors = new NearestNUserNeighborhood(getHowMany(), similarity, model);
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;但是推荐的结果不尽人意。想到之前微博推荐的经验， 可以先对book根据book-tag matrix&lt;strong&gt;聚类&lt;/strong&gt;，再在每个cluster里找邻居，这样可以消除一些离群点的影响。看结果：
&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/rec_douban_tag.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
可以看到推荐的结果有&lt;a href=&quot;http://book.douban.com/subject/3644791/&quot;&gt;噪音太多&lt;/a&gt;、&lt;a href=&quot;http://book.douban.com/subject/1831760/&quot;&gt;弱水三千&lt;/a&gt;...&lt;/p&gt;

&lt;h4&gt;协同过滤&lt;/h4&gt;

&lt;p&gt;协同过滤， 说白了也是在people-book matrix里找邻居。不同的是user-based collaborative filtering是把book当成people的dimension。而item-based collaborative filtering则反过来。这里仍然用&lt;em&gt;PearsonCorrelationSimilarity&lt;/em&gt;来衡量相似度：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
NearestNUserNeighborhood neighbors = new NearestNUserNeighborhood(numNeighbor, similarity, model);
Recommender recommender = new GenericUserBasedRecommender(model, neighbors, similarity);
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;很遗憾没有爬到自己的账号。随便选个账号来测试，结果：
&lt;br /&gt;
&lt;br /&gt;
item-based
&lt;img src=&quot;/images/recommendation/rec_douban_item.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
user-based
&lt;img src=&quot;/images/recommendation/rec_douban_user.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
因为爬到的数据不多， 本来就稀疏的matrix交集更少。推荐出来的结果都是些可能看过一两本相同书的people给出的推荐。这也反映出协同过滤的cold start问题。不知道豆瓣是怎样解决的。
&lt;br /&gt;
&lt;br /&gt;
同上，我也试着对people根据people-tag matrix先聚类再做item-based collaborative filtering。推荐出来的比较像tag-based的结果。&lt;/p&gt;

&lt;h4&gt;new book recommendation&lt;/h4&gt;

&lt;p&gt;豆瓣还有个推荐新书的功能。根据爬到的数据，我能想到的方法就是计算people的历史tag跟新书tag之间distance，选距离最近的people。但是这时的距离应该是什么呢？ EuclideanDistance？试了下， 结果比较无语，因为推荐出来的都是些过分活跃的用户-那些所有tag都有比较大的weight的用户。
&lt;br /&gt;
&lt;br /&gt;
其实这也可以理解， content based必然会有热门问题，比如豆瓣的tag-based推荐，像&lt;em&gt;文学&lt;/em&gt;、&lt;em&gt;小说&lt;/em&gt;这样的tag就比较泛滥。 所以很多文学类小说的推荐常常会被几本热门的名著占据。 按理说豆瓣应该会对tag清理过，不明白为什么还是会有这样的问题。
&lt;br /&gt;
&lt;br /&gt;
显然要换个距离， 欧式距离会计算所有tag间的距离，是否可以让距离只考虑新书的tag在people上的weight。想到了对people向量做横向归一处理，这样就消除了热门tag对people的影响，也就能找到新书的tag占最大weigh的people。 看结果:
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/rec_douban_new.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
这样就为&lt;a href=&quot;http://book.douban.com/subject/1041007/&quot;&gt;哈利·波特与魔法石&lt;/a&gt;找到了用户&lt;a href=&quot;http://book.douban.com/people/tuzicxy/collect&quot;&gt;tuzicxy&lt;/a&gt;、&lt;a href=&quot;http://book.douban.com/people/70621573/collect&quot;&gt;70621573&lt;/a&gt;...&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Aug 2013 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2013/08/16/recommendation-douban/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2013/08/16/recommendation-douban/</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>hadoop sort问题</title>
        <description>&lt;p&gt;项目组最近遇到一个hadoop(1.2)问题,因为map中产生的key太多,导致job花费了大量时间在map的shuffle上。准确来说时间应该都花在了IO上面，map产生的数据过多，使得shuffle阶段IO操作过多。 当然因为db设计的原因，map数据过大也是必然的事。  纵观Mapreduce的shuffle过程： sort、 IO 、merge， 似乎也只有sort还有优化的可能。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
这里先跑下题 Mapreduce本是一个聚合的过程, 但是因为后台db采用了星形的架构 使得map将数据发散到一个更广的空间中。 key多的点让人觉得不可理喻。 或许这种设计本身就不适合用Mapreduce来做ETL。
&lt;br /&gt;
&lt;br /&gt;
言归正传，于是项目组决定对Mapreduce的排序进行优化。 下面是优化的笔记：
&lt;br /&gt;
&lt;br /&gt;
Mapreduce的shuffle过程包括map阶段和reduce阶段。   在map阶段， 如图， 首先将输出写到缓存当中。 当缓存大小达到“阈值”时（默认的大小是缓存的80%），  变会进行&#39;spill&#39;： 即sort and combiner。  sort完才会把数据写进磁盘。  坑爹的是，  根据Mapreduce的设计(hadoop1.2)， sort是个无法避免的过程。
&lt;br /&gt;
&lt;img src=&quot;/images/hadoop/map_shuffle.jpg&quot; alt=&quot;map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在sort的过程中， 由于hadoop往缓存中写入的不是对象， 而是序列化以后的二进制数据。 所以在WritableComparator中， 会首先将这些二进制数据反序列化为对象， 然后再调用这些对象的compareTo方法进行比较：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
  try {
    buffer.reset(b1, s1, l1);                   // parse key1
    key1.readFields(buffer);
  
    buffer.reset(b2, s2, l2);                   // parse key2
    key2.readFields(buffer);
  
  } catch (IOException e) {
    throw new RuntimeException(e);
  }
  return compare(key1, key2);                   // compare them
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;所以说， 每次compare的过程都会有两个反序列化的过程。 当然还好hadoop留了一条后路， Job对象上有一个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setSortComparatorClass(RawComparator c)
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;这样我们就能避免反序列化过程了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleComparator implements RawComparator{
  private Logger logger = Logger.getLogger(SampleComparator.class);

  @Override
  public int compare(Object o1, Object o2) {
    logger.error(&quot;SampleComparator does not support method compare(Object o1, Object o2)!&quot;);
    throw new RuntimeException(&quot;unsupported method!&quot;);
  }

  @Override
  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
    return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
  }
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;当然， 进行这种修改的时候一定要确保是否序列化对排序结果没有影响。&lt;/p&gt;

&lt;h5&gt;附&lt;/h5&gt;

&lt;p&gt;顺便记一下最近看到的一个问题：
 为什么hadoop不适合做实时计算？ 个人的感觉主要是IO太多， 再加上启动时间、 网络传输等因素 ，就决定了hadoop在实时这个领域难有作为。
&lt;br /&gt;storm就不一样，数据都是在内存中， task一直在监听数据， 除非kill掉(下次系统看下storm)&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jul 2013 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2013/07/14/hadoop-sort/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2013/07/14/hadoop-sort/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>machine learning 笔记</title>
        <description>&lt;p&gt;接触&lt;strong&gt;machine learning&lt;/strong&gt;一年多了 也攒了不少笔记 话说期间各种算法都有接触过 分类 聚类  推荐 降维...  或许是research的不够深入的原因吧  我发现其实大多数算法都可以用一句话概况&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;分类&lt;/h4&gt;

&lt;h5&gt;Bayesian&lt;/h5&gt;

&lt;p&gt;最有可能属于哪一类
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/bayesian.jpg&quot; alt=&quot;bayesian&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;SVM&lt;/h5&gt;

&lt;p&gt;找出一个能划分两个类的超平面
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/svm.jpg&quot; alt=&quot;SVM&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Decision tree&lt;/h5&gt;

&lt;p&gt;每个枝上选一个维度去划分节点上的所有样本
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/dt.jpg&quot; alt=&quot;decision tree&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;ANN&lt;/h5&gt;

&lt;p&gt;函数黑盒  这个似乎有点高深  现在也不知道能不能再推出来
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/ann_1.jpg&quot; alt=&quot;ANN&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/ann_2.jpg&quot; alt=&quot;ANN&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;距离&lt;/h4&gt;

&lt;p&gt;各种距离  最常用的是欧式和余弦  可能根据应用场景的不同有些差异  比如 点 p(5, 5), p(1, 1) 和 p(0, 0)之间的距离。 这种差异可以通过归一化来handle
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/dis.jpg&quot; alt=&quot;distance&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;聚类&lt;/h4&gt;

&lt;h5&gt;kmeans&lt;/h5&gt;

&lt;p&gt;最基础的聚类算法  &lt;em&gt;选中心点&lt;/em&gt;  &lt;em&gt;不停的迭代&lt;/em&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/kmeans.jpg&quot; alt=&quot;kmeans&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Spectral&lt;/h5&gt;

&lt;p&gt;谱聚类
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/spectral_1.jpg&quot; alt=&quot;spectral&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/spectral_2.jpg&quot; alt=&quot;spectral&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;降维&lt;/h4&gt;

&lt;p&gt;LDA是监督学习  将不同的样本映射到&lt;em&gt;sub space&lt;/em&gt;上  使得不同label之间的方差最大化 但是同一label内的样本尽量靠近各自的中心
&lt;br /&gt;
PCA是无监督学习  也是将所有样本映射到&lt;em&gt;sub space&lt;/em&gt;上 不同的是降维后使得所有样本的方程最大化
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/pca_1.jpg&quot; alt=&quot;pca&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/pca_2.jpg&quot; alt=&quot;pca&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;推荐&lt;/h4&gt;

&lt;p&gt;基于内容的  基于协同的  还有关联规则的&lt;/p&gt;

&lt;h5&gt;关联规则&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;支持度&lt;/em&gt;  &lt;em&gt;置信度&lt;/em&gt; &lt;em&gt;频繁项集&lt;/em&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/asso.jpg&quot; alt=&quot;asso&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;条件概率&lt;/h5&gt;

&lt;p&gt;阿里技术博客上看到的 很简单的based on&lt;strong&gt;条件概率&lt;/strong&gt;的 推荐
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/rec_ali.jpg&quot; alt=&quot;ali&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Note&lt;/h4&gt;

&lt;p&gt;不记得哪里看到的一条机器学习的三维
&lt;br /&gt;
&lt;img src=&quot;/images/note/ml/ml_3.jpg&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Jul 2013 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2013/07/10/note-ml/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2013/07/10/note-ml/</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>排序算法笔记</title>
        <description>&lt;h4&gt;文章排名&lt;/h4&gt;

&lt;p&gt;最近工作中需要对一个CMS中的post排序进行优化, 所以临时学习了下各种文章排名算法。 而影响排序的因素无非是时间、总评数、好评、差评    ，怎么倾向则根据具体business来定。 比如电商可能会选择忽略时间效应的wilson score， 而像Hacker News就更可能会把时间考虑的比较重要。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;
上笔记吧：
&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/note/algorithm/sort_2.jpg&quot; alt=&quot;排名算法&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;排序基础&lt;/h4&gt;

&lt;p&gt;顺便复习下基础的排序算法。 看过去的笔记总觉得是件开心事。&lt;/p&gt;

&lt;h5&gt;冒泡、插入、选择&lt;/h5&gt;

&lt;p&gt;这三个排序都需要两遍循环array， 所以时间复杂度在最坏情况下都是O(n&lt;sup&gt;2&lt;/sup&gt;), 也相对比较简单。
&lt;br /&gt;
&lt;img src=&quot;/images/note/algorithm/sort_1.jpg&quot; alt=&quot;排序基础&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;快速、归并&lt;/h5&gt;

&lt;p&gt;keyword: &lt;em&gt;Divide-and-Conquer&lt;/em&gt;、 &lt;em&gt;空间换时间&lt;/em&gt; 、&lt;em&gt;O(nlogn)&lt;/em&gt;
&lt;br /&gt;
Quick sort：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void quick(int[] ary, int start, int end) {
    if (start &lt; end) {
        int index = partition(ary, start, end);
        quick(ary, index + 1, end);
        quick(ary, start, index - 1);
    }
}
int partition(int[] ary, int left, int right) {
    int temp = ary[left], index = left;
    swap(ary, index, right); // 基准点后置
    for (int i = index; i &lt; right; ++i) {
        if (ary[i] &lt; temp)
            swap(ary, index++, i);
    }
    swap(ary, index, right); // 换回基准点
    return index;
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;Merge sort：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void merge(int a[], int first, int last) {
    int temp[] = new int[a.length];
    if (first &lt; last) {
        int mid = (first + last)  &gt;&gt; 1;
        merge(a, first, mid);
        merge(a, mid + 1, last);
        memeryArray(a, first, mid, last, temp);
    }
}

void memeryArray(int a[], int first, int mid, int last,
        int temp[]) {
    int i = first, j = mid + 1;
    int m = mid, n = last;
    int k = 0;

    while (i &lt;= m &amp;&amp; j &lt;= n) {
        if (a[i] &lt;= a[j])
            temp[k++] = a[i++];
        else
            temp[k++] = a[j++];
    }

    while (i &lt;= m)
        temp[k++] = a[i++];

    while (j &lt;= n)
        temp[k++] = a[j++];

    for (i = 0; i &lt; k; i++)
        a[first + i] = temp[i];
}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Wed, 03 Jul 2013 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2013/07/03/note-sort/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2013/07/03/note-sort/</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>上海野生动物园</title>
        <description>&lt;p&gt;上海野生动物园, 强烈推荐啊！&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;似乎被一直长尾猴鄙视了...
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/changweihou.jpg&quot; alt=&quot;长尾猴&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;好可爱
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/changweihou_2.jpg&quot; alt=&quot;长尾猴&quot; /&gt;
&lt;img src=&quot;/images/travel/zoo/daishu.jpg&quot; alt=&quot;袋鼠&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;进击的金丝猴
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/jinsihou.jpg&quot; alt=&quot;金丝猴&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;胖子中的战斗机
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/daxiang.jpg&quot; alt=&quot;大象&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;见到传说中的神兽了
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/shengshou.jpg&quot; alt=&quot;神兽&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;会跳舞的熊 方便面君 小丁满
&lt;br /&gt;
&lt;img src=&quot;/images/travel/zoo/bear.jpg&quot; alt=&quot;bear&quot; /&gt;
&lt;img src=&quot;/images/travel/zoo/huanxiong.jpg&quot; alt=&quot;方便面君&quot; /&gt;
&lt;img src=&quot;/images/travel/zoo/pipi.jpg&quot; alt=&quot;丁满&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Mar 2013 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2013/03/09/zoo/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2013/03/09/zoo/</guid>
        
        
        <category>travel</category>
        
      </item>
    
      <item>
        <title>十一黄山看人记</title>
        <description>&lt;p&gt;黄山是所有爬过的山中最漂亮的！&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;img src=&quot;/images/travel/yellow_mountain/y_1.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_2.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_3.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_4.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_5.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_6.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_8.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/travel/yellow_mountain/y_7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Oct 2012 00:00:00 +0800</pubDate>
        <link>http://dannyhnu.github.io/2012/10/01/yellow_mountain/</link>
        <guid isPermaLink="true">http://dannyhnu.github.io/2012/10/01/yellow_mountain/</guid>
        
        
        <category>travel</category>
        
      </item>
    
  </channel>
</rss>
