<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dan</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 05 May 2019 17:11:50 +0800</pubDate>
    <lastBuildDate>Sun, 05 May 2019 17:11:50 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>spark vs. flink</title>
        <description>&lt;p&gt;先说观点，spark和flink都是目前能够一站式解决lambda架构的计算框架。因为spark出来的早，现在在活跃度、成熟度方面比flink强。flink发展相对较晚，相比spark各个细节做了很多优化。个人觉得二者是同一个世代计算框架的不同实现，细节方面会相互学习、增强。但要说谁能干掉谁，恐怕都没有这能力。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;这里从几个方面比较下二者的区别。&lt;/p&gt;

&lt;h3&gt;内存管理&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SPARK. executor内存管理算是spark的一个软肋。spark并不会直接操操控jvm上堆内内存，只是进行逻辑上的&quot;规划式&quot;的管理。
当jvm起来的时候，executor会维护Storage、Execution的堆内堆外四个内存&lt;em&gt;map&lt;/em&gt;, 分别记录各个块在堆内堆外的内存状态。虽然会有一些防止oom的buffer机制，但这种内存记录器不可能完全准确(比如被spark标记释放的对象可能没有被JVM回收)，所以spark不能完全避免OOM。
&lt;img src=&quot;/images/hadoop/spark_mem_1.png&quot; alt=&quot;note&quot; /&gt;
&lt;img src=&quot;/images/hadoop/spark_mem_2.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FLINK. 相比spark，flink在内存管理更用心。&lt;br&gt;
① 采用积极的内存管理策略：用专门的内存池负责每个数据片的分配和管理。&lt;br&gt;
② 定制化的数据片操作：高效的二进制操作、量身定制的序列化框架。&lt;br&gt;
&lt;img src=&quot;/images/hadoop/flink_mem_1.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;抽象&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据模型. 对于底层数据抽象，rdd(Resilient Distributed Datasets)是spark的灵魂。rdd其实是一种分布式的内存抽象，负责处理内存、磁盘数据的分布式存储、和HA(checkpoints)等。spark在rdd上封装出dataset，不管是batch还是streaming都直接通过dataset操控数据。而flink的底层数据抽象成一种有状态流(stateful stream), 然后分别封装成DataStream和Dataset供streaming和batch任务操作数据。在业界普遍无缝结合batch、streaming任务的趋势下，spark的做法显然更合理点。但是flink考虑到了实时计算的上下文状态的重要性，这点更胜一筹。
&lt;img src=&quot;/images/hadoop/flink_data.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;编程模型. 对于数据处理流，spark和flink本质上是一样的。虽然叫法不一样(spark叫job、stage，flink叫transformation、operator)，但其实都是map-&gt;shuffle-&gt;reduce的dag(有向无环图)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;实时计算&lt;/h3&gt;

&lt;p&gt;在实时计算(streaming)的支持上，flink做的更出色一点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;窗口. spark对窗口的支持比较弱，只能根据time interval起batch任务。flink对窗口的支持很强大，既能按时间驱动(比如：每30秒)，也能按数据驱动(比如：每100个元素)。而且窗口的类型包括滚动窗口（没有重叠）， 滑动窗口（有重叠），以及会话窗口 （由不活动的间隙所打断）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;状态. spark streaming不记录数据状态，所以对于一些上下文相关的计算，spark需要借助外部的存储来保存上下文状态。而flink提供了State Backends对历史数据进行保存，比如存在内存(MemoryStateBackend)、文件（FsStateBackend）、外部DB(RocksDBStateBackend)，这样会使得一些上下文相关的计算逻辑更易实现。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;应用层&lt;/h3&gt;

&lt;p&gt;应用层因为spark发展的时间比较久，社区也更活跃点，所以在sql、机器学习的支持都已经做的比较成熟了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SQL. 对flink而言，sql层统一了batch和streaming的编程模型。但是由于支持比较晚，各方面还不是很成熟。二者还有一点不同的是，spark的的底层sql解析器是Catalyst，而flink是Calcite。当然，这个差别并没有什么大的意义。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ML：同上，相比spark，flink还有很长的路要走。但是相比spark，flink有个天生的优势。spark由于rdd的不可变性，且不支持状态存储，所以在机器学习的迭代过程中显得力不从心。而flink由于支持状态持久化，所以在算法迭代计算过程中会更方便。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;附录&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html&quot;&gt;Apache Spark 内存管理详解&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://wuchong.me/blog/2016/04/29/flink-internals-memory-manage/&quot;&gt;Flink 原理与实现：内存管理&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/12/spark_vs_flink/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/12/spark_vs_flink/</guid>
        
        
        <category>realtime</category>
        
      </item>
    
      <item>
        <title>大数据笔记 - 监控系统</title>
        <description>&lt;p&gt;在实际工作中经常碰到这样的监控需求，重点城市的访问流量是否有大的波动、一个广告位的ecpm是否在合理的范围内、机器的CPU利用率是否正常...
经过一段时间case by case的支持，我们想到或许有通用的办法去解决这类问题。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;h3&gt;overview&lt;/h3&gt;

&lt;p&gt;简单来说，各类监控需求都可以总结为判断一条曲线是否正常，横轴是时间，纵轴是PV、单价、CPU利用率等等的曲线。所以我们对监控流程做了这样的改进：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;老流程：产品、运营、开发手工指定监控维度，并根据经验拍板监控规则。这样带来的问题就是监控维度覆盖面比较低、监控规则指定随意、滞后、维护成本高。
&lt;img src=&quot;/images/ops/process_old.PNG&quot; alt=&quot;note&quot; /&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;新流程：自动扫描监控维度，自动学习监控规则。
&lt;img src=&quot;/images/ops/process_new.PNG&quot; alt=&quot;note&quot; /&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;自动化监控&lt;/h3&gt;

&lt;p&gt;监控流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1 自动扫描监控维度(单维或交叉维度)&lt;/li&gt;
&lt;li&gt;2 对监控时间序列曲线进行分类(平稳、自相似、非平稳)&lt;/li&gt;
&lt;li&gt;3 对不同类别曲线采用不同监控规则(组合策略)&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;时间序列的自相似性&lt;/h4&gt;

&lt;p&gt;在对曲线分类之前，先介绍下我们对时间序列自相似的定义：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算公式：对于时间序列t我们按周期长度n划分序列t={t1,t2…tm}, 其中m是周期数。我们采用欧式距离标准差来度量周期间的距离，用若干周期内距离平均值来度量自相似性：
  d(ti) = stdev(linearScale(ti))， 0 &amp;lt; i &amp;lt; n
  sim(t) = mean(d(t1),d(t2)..d(tn))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/ops/sim.png&quot; alt=&quot;note&quot; /&gt;&lt;br&gt;
当一条曲线在不同周期内的距离越小，我们认为这条曲线自相似性越高。&lt;br&gt;
&lt;img src=&quot;/images/ops/dis.PNG&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;序列分类&lt;/h4&gt;

&lt;p&gt;我们采用以下几个步骤对曲线分类为平稳、自相似、非平稳时间序列：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;平稳曲线： 计算曲线标准差，当波动小于某个阈值，我们认为这条曲线为平稳时间序列。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;自相似曲线: 根据监控时效性对曲线划分周期，计算曲线自相似距离，当自相似距离小于某个阈值为自相似曲线。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;非平稳曲线: 不满足以上两个条件。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;监控策略 · 常规监控&lt;/h4&gt;

&lt;p&gt;对于平稳时间序列的监控(这类指标数据一般波动较小 ，而且表现出&lt;em&gt;强业务特性&lt;/em&gt;)，我们多采用比较常规的监控策略。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;同环比&lt;/p&gt;

&lt;p&gt;  环比 : f(t)/f(t-1)&lt;br&gt;
  同比 : f(t)/f(t-c)。c为周期，一般为三分钟、7天、30天等。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;动态阈值&lt;/p&gt;

&lt;p&gt;  历史极值 ：取历史指定若干周期的数据的极值，差值超过 三倍方差 则判定异常。&lt;br&gt;
  移动平均 ：取历史指定若干周期的数据，去除极大、小值(消除历史异常点的影响)取平均。差值超过 三倍方差 则判定异常。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EWMA
  指数加权移动平均 ：取历史指定若干周期的数据，去除极大、小值(消除历史异常点的影响)：&lt;br&gt;f(t)=αf(t−1)+(1−α)θt&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;监控策略 · 时间序列基线&lt;/h3&gt;

&lt;p&gt;自相似时间序列监控主要面向的业务场景是 分钟级 的流量监控（ 波动大，而且可能极不规律 ）。基于时间序列预测的基本想法是：我们用监控维度的过去 若干个周期 的时间序列去预测下一个周期，用 预测 出来的时间序列作为基线，然后拿当天的时间序列和基线进行 比较 ，超过一定范围则触发告警。解释下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分钟级: 我们对流量数据按分钟切片，如果有业务波动可以在分钟内发现。&lt;/li&gt;
&lt;li&gt;若干个周期: 我们忽略了季节性等因素，只考虑了一周之内的波动。所以选取了7|14天为历史周期。&lt;/li&gt;
&lt;li&gt;预测: 预测方法用的是EWMA和Holt winters。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;对于相邻周期内相似的时间序列(周期内的波动比较稳定)，通过一般的时间预测方法都能预测出比较好的时间基线。以新闻广告的列表信息流为例，基线和实际序列的差值可以稳定在一定的范围内：
&lt;img src=&quot;/images/ops/ts_1.png&quot; alt=&quot;note&quot; /&gt;
    对于基线和实际序列的偏差阈值的选择有两个办法：3sigma和局部密度估计(Local Outlier Factor)。&lt;/p&gt;

&lt;p&gt;对于相邻周期内不相似的时间序列，比较直接的想法是 转换成自相似时间序列 。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拉宽时间窗口
以新闻插件广告位为例，由于新闻push会带来流量的飙升，而每天push的时间点又不一样，所以流量的峰值在每天都不太相同。所以分钟级的基线预测不太理想： &lt;img src=&quot;/images/ops/ts_2.png&quot; alt=&quot;note&quot; /&gt;
如果将时间窗口拉大，比如以两个小时为例，则可以减小预测误差：
&lt;img src=&quot;/images/ops/ts_3.png&quot; alt=&quot;note&quot; /&gt;
拉宽时间窗口其实是 牺牲了时效性来平滑时间序列 ，所以通常不太推荐这个方法。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;差分时间序列：这里说的差分包括并不限于对时间序列作n阶差分、转换为增长率(同比或者环比)序列。
以新闻banner广告为例，由于每天的客户数波动较大，所以广告曲线的振幅也不一样。
&lt;img src=&quot;/images/ops/ts_4.png&quot; alt=&quot;note&quot; /&gt;
但是每天曲线的升降趋势却相似，所以转换成变化速度曲线：f`(t) =( f(t)- f(t-n) )/ f(t-n), n为时间跨度。
&lt;img src=&quot;/images/ops/ts_5.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;监控策略 · 相似波&lt;/h4&gt;

&lt;p&gt;由于广告业务的多样性和媒体的推送操作的不确定性(新闻每天不定时的push)，会导致很多维度的广告PV曲线并没有自相似性甚至毫无规律。
 &lt;img src=&quot;/images/ops/ts_6.png&quot; alt=&quot;note&quot; /&gt;
对于这类非平稳曲线，我们监控的主要目标是 波动特性是否在合理的范围 内。所以我们抽取出曲线的波，通过和历史的正常波形进行比较来判断是否异常。&lt;/p&gt;

&lt;h4&gt;波的度量&lt;/h4&gt;

&lt;p&gt; 我们认为一个波的生命周期从上升率大于某个特定值为波的 上升期 ，到达 波峰 后变化率变为负进入 下降期 ，当下降率恢复到某个特定值则为到达 波谷 。在一个波的完整生命周期内我们会抽取出这个波的一些度量：&lt;br /&gt;
* 上升率 ： 波在上升期的增长率&lt;br /&gt;
* 波峰 ： 波的最高值&lt;br /&gt;
* 上半生波长 ：上升期的波长&lt;br /&gt;
* 下半生波长 ：下降期的波长&lt;br /&gt;
* 下降率 ：波在下降期的下降率&lt;br /&gt;
* 波谷 ：波的最低值&lt;br /&gt;
 为了平滑小区间内的波动，我们滑动一个固定宽度的时间窗口。把一个窗口内的点 拟合成一条直线 。&lt;/p&gt;

&lt;h4&gt;波的监控&lt;/h4&gt;

&lt;p&gt; 在波不同的生命周期内我们会触发不同的监控策略，最后组合多个策略判断波动是否异常：&lt;br /&gt;
* 上升率 ：上升期内上升率如果超过历史正常上升率的极大极小值，则标记异常。&lt;br /&gt;
* 波峰 ： 波峰和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 上半生波长 ：上半生波长和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 下半生波长 ：下半生波长和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;
* 下降率 ：下降期内下降率如果超过历史正常下降率的极大极小值，则标记异常。&lt;br /&gt;
* 波谷 ：波谷和历史均值的差值超过3个标准差，则标记异常。&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Nov 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/11/20/shark-monitor/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/11/20/shark-monitor/</guid>
        
        
        <category>ops</category>
        
      </item>
    
      <item>
        <title>大数据笔记 - 实时olap</title>
        <description>&lt;p&gt;最近半年花了不少时间在实时olap上，业务场景是全维度实时监控，这里记录下自己的一些思考。
对于大多数大数据底层应用，公司都有很成熟的支持中间件。比如实时消息队列(TDBANK)、KV存储(CKV)、sql on hadoop(tdw)等等。可是实时olap这个场景，基本上都是各个业务自己小规模的玩玩。之前一直想吐槽数据平台不作为，现在自己摸索了这么久才终于明白，实时olap场景的投入产出比 -- 真的有点低。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;大数据场景下的实时olap，说简单点就是极致的内存计算，极小的磁盘、网络IO。把那么多的数据全堆到内存，听起来都觉得内存和CPU在燃烧。但是实时查询的实际需求又没那么强烈。像弱olap实时查询场景：比如淘宝实时交易额、实时在线人数，其实都可以通过kv系统解决，没必要把所有数据都放内存。而全维度olap场景，sql on hadoop(spark、kylin、impala)可以很好的做到分钟级响应(秒级真有那么重要？)。
话虽如此，但总有些绕不过去的硬需求需要真正意义上的实时olap。
&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;OVERVIEW&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;大数据场景下的olap，区别于大多数sql on hadoop解决方案(hive,spark,impala...),实时oalp(click house,druid,pinot,Vertica...)可以真正意义上做到秒级响应。这主要是因为：&lt;br&gt;
① 定制化的数据存储、压缩、索引，可以极大的减少磁盘IO，增加内存计算。&lt;br&gt;
② 类MPP架构，节点间极少的数据交换，网络IO最小化。&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当然，这也会带来一些限制：&lt;br&gt;
① 特定的存储使得数据更新变得麻烦，所以很难做到数据的更新和细化删除。&lt;br&gt;
② 不支持节点间数据交换的复杂的计算，比如大表间的join、UV计算(一般是近似计算)。&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;最近半年在技术选型的时候稍微了解了下业绩的不少开源方案(click house,druid,pinot)，这里记录下click house和druid在各个环节的异同。&lt;/p&gt;

&lt;h3&gt;架构&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;druid： druid的架构从某种程度来说更分布式，更'MPP'。不同的NODE负责不同的工作，且node都采用master-slave架构。所以druid在大集群下的HA表现更好，当然运维部署也会更复杂点。
&lt;img src=&quot;/images/olap/druid.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;click house：ck从某种意义上是可扩展单机DB，专注于单机上的性能压榨，然后在meta记录集群其他节点信息。这样在运维部署相对简单，但应用层也需要做更多分布式的工作，比如入库自己需要做负载均衡。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;数据入库&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;druid：druid的数据入库相对ck来说有点重，不管是对数据约束还是服务运维。druid支持离线和在线两种入库方式，离线可以和hadoop无缝结合，实时则有专门的realtime node负责数据接入。&lt;/li&gt;
&lt;li&gt;click house：ck则相对简单，数据入库和传统DB类似，比如jdbc连接。但是因为简单，所以应用层需要充分考虑集群的负载。&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;数据存储&lt;/h3&gt;

&lt;p&gt;毫无意外，这类db都会采用列存的存储格式。因为列存可以更好的压缩数据，而且在查询时也能节省非必要的解压IO。为了在查询时快速定位数据，ch和druid都对数据作了索引。不过在&lt;em&gt;数据压缩&lt;/em&gt;、&lt;em&gt;数据索引&lt;/em&gt;、&lt;em&gt;数据备份&lt;/em&gt;方面，二者有些小的差别。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;druid：druid的存储文件格式叫segment。包含timestamp、维度(Dimensions)、指标(Metrics)三部分。segment文件采用LZ4压缩，另外还会维护一个string-&gt;int的隐射表，避免了文件中有重复的string字符串。这样做是因为int的压缩效率比string几乎多一个量级。在索引方面，druid会建个bit map做倒排，这对某些filter场景可以减少很多IO。在数据备份方面，druid有一个高效的balance策略把每个segment存储到不同节点。而且有master记录、监控每个segment的状态。
&lt;img src=&quot;/images/olap/segment.png&quot; alt=&quot;note&quot; /&gt;
&lt;img src=&quot;/images/olap/druid_compresion.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;click house：ck按分区存储数据，定时对分区进行merge。默认用ZSTD压缩。索引只采用了比较简单的主键稀疏索引。稀疏索引虽然更容易做数据压缩，却不能很好的减少数据读取。所以ck更多时候是做全表扫描，但是由于对主键排序和借助&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E5%8D%95%E6%8C%87%E4%BB%A4%E6%B5%81%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%B5%81&quot;&gt;SIMD&lt;/a&gt;,ck的读取效率其实非常高。数据备份ck做的比较静态，每个server会记录哪台server是自己的备份。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;结论&lt;/h3&gt;

&lt;p&gt;这里我贴出&lt;a href=&quot;https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7&quot;&gt;大牛的对比&lt;/a&gt;。简单来说，在数据和集群规模不大的情况下，ck是一个更好的选择。
&lt;img src=&quot;/images/olap/compare.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;附录&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7&quot;&gt;Comparison of the Open Source OLAP Systems for Big Data: ClickHouse, Druid and Pinot&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Aug 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/08/11/realtime_olap/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/08/11/realtime_olap/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>大数据笔记 - 大数据计算框架</title>
        <description>&lt;p&gt;大数据计算框架日新月异，而每次计算效率的大幅提升，简单总结就一句话：IO优化。不管是存储，还是计算的优化，都是更少的磁盘IO，更多的内存IO。本文结合本人在自身工作中的经验，主要讲一下对大数据计算效率上的一些思考。&lt;br&gt;&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;img src=&quot;/images/hadoop/framework.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;技术选型&lt;/h2&gt;

&lt;p&gt;对大数据查询平台的技术选型，可以从存储和计算两个方面结合自身业务来衡量。&lt;/p&gt;

&lt;h3&gt;存储&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;扫表vs 索引：建索引可以快速locate需要查询的数据，完全省去了对查询不相关数据的读写IO。但是代价却是漫长的建索引过程和大量额外的存储开销。所以建索引适合经常带filter的数据筛选计算和数据维度基数较多的业务场景。对于一些不带filter的查询和大范围时间内的聚合计算业务场景，建索引就显得多余了&lt;/li&gt;
&lt;li&gt;列存vs 行存：相比于传统DB的行存，列存能大量减少磁盘IO。比如更高效的数据压缩、查询不需要解压非相关字段、更好的索引策略等。但是在数据更新场景下，列存会带来一些额外的开销。所以对更新频繁的业务场景，选择列存就要慎重了。&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;计算&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Map reduce vs MPP: 个人觉得二者的边界比较模糊。Map reduce架构扩展性强、可用性高。MPP由于一些不共享约束，使得计算效率相比而言更高(多内存IO，当然对硬件要求也更高)。所以个人觉得，对于一些离线、近实时处理Map reduce架构更可靠(如hive、spark)。而实时计算，多采用MPP架构（比如业界比较流行的计算框架GP、druid、clickhouse等）。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;数据预处理&lt;/h2&gt;

&lt;p&gt;对于数据计算过程最耗IO的两个步骤：JOIN和GROUP BY，可以在数据预处理阶段节省计算过程的IO。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;预JOIN：大宽表设计，可以省掉计算过程的join IO，这个可能是比较常用的操作，就不多赘述。可能需要考虑的一点是对于经常变动的业务字段，不太适合预JOIN，因为重跑历史数据真的是非常头疼。所以预join的字段最好是不常变动的。&lt;/li&gt;
&lt;li&gt;数据CUBE：预先group by，简单来说就是空间换时间。apache kylin运用的就是这个方法。但是在海量、高维度的业务场景下，野蛮预建cube的存储代价太大。所以我们提出一种基于组合优化的物化cube智能生成方法：
① 从L(n)出发，计算得到满足覆盖率t的所有n-1规模集合L(n-1), 进一步迭代 L(n-2), L(n-3), ...L(n-k), 删选出所有满足最低覆盖率t的组合C；&lt;br&gt;
② 计算每个组合Ci的空间占用Vi 和收益Ri[(r1/r2)*Coverage(pattern)]；&lt;br&gt;
③ 转化为0\1背包问题：在满足空间限制V的情况下，选出最大收益R的集合Q。&lt;br&gt;
简单来说，就是根据历史查询选出综合考虑查询覆盖和空间占用的最佳组合。下图是查询是否命中cube的时间对比。&lt;br&gt;
&lt;img src=&quot;/images/hadoop/cube.PNG&quot; alt=&quot;note&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;查询计划优化&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;分区筛选：根据业务特点可对用户查询添加特定分区筛选条件。举一个我们业务场景的case，当用户查特定广告排期(非分区字段)，我们在查询计划加上了筛选该排期下订单(分区字段)的条件，这样省略了大量非必要分区的读写IO。&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 20 Jun 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/06/20/shark/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/06/20/shark/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>大数据笔记 - olap</title>
        <description>&lt;p&gt;OLAP(联机分析处理)只是一个概念，但背后是一个数据系统，包括数据接入、数据仓库的建设、数据服务(ad hoc查询、可视化、数据挖掘...)。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;数据仓库&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据分层：数据为什么要分层存储？&lt;br&gt;
① 原始数据和应用数据的解耦，使得数据结构更加清晰，也能使得原始业务和应用层的变动不会相互影响。&lt;br&gt;
② 减少重复的数据开发，使得很多对外的数据输出可以复用，也能保证对外数据的一致性。&lt;br&gt;
至于怎么分层，业界比较常见的分层方法是：ods（原始日志层）-&gt; dw（数据仓库层）-&gt; dm（数据集市层）-&gt; app（数据产品层）。个人觉得还是看业务本身的复杂性，根据以往的经验，层数往往和运维的复杂度成正比。所以没有必要为了分层而分层，简单好用才是王道。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据模型：传统DB可能多采用关系建模(ER建模)，严格符合3范式。这种约束对于在大数据的场景可能过于严格，所以大数据仓库多采用维度建模。而维度模型最常见的是：
① 星形模型。事实表+一级维度表，可读性好，易维护，聚合查询效率高。&lt;br&gt;
② 雪花模型。事实表+多级维度表。数据冗余少，后期维护较复杂。&lt;br&gt;
对于olap查询系统，个人倾向于星形模型，简单易用，可以省去很多麻烦(所以kylin才会只支持星形)。贴一下网上找的模型对比:&lt;br&gt;
&lt;img src=&quot;/images/hadoop/model.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;元数据(meta)管理：&lt;br&gt;
1) 数据分区？&lt;br&gt;
2) 数据cube？&lt;br&gt;
3) 数据视图？&lt;br&gt;
4) 历史回溯？&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;ETL // TODO&lt;/h3&gt;

&lt;h3&gt;olap框架 // TODO&lt;/h3&gt;

&lt;h3&gt;附录&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/u010454030/article/details/74589791&quot;&gt;理解数据仓库中星型模型和雪花模型）&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Sep 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/09/21/olap/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/09/21/olap/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>ALGO笔记 - LR</title>
        <description>&lt;p&gt;LR一般用来解决二分类问题，表达能力比较弱，可以简单的这样理解：LR是一个拟合因和果的算法，它认为任何结果y必定和一系列因x存在某些关系w。虽然比较朴实，但LR真的是一个万金油的算法，我们在很多工作中(CTR预估、游戏推荐、系统运维...)都用到了。因为LR：&lt;br&gt;&lt;/p&gt;

&lt;!-- more --&gt;


&lt;ol&gt;
&lt;li&gt;简单、有效(数据量大到一定程度，这些模型的效果其实差别并不大), 可解释性好。&lt;/li&gt;
&lt;li&gt;简单易部署，分布式训练算法比较成熟(在大数据场景下，训练速度快、消耗资源少)，适合online learning。&lt;/li&gt;
&lt;li&gt;稳定。&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;基本假设&lt;/h3&gt;

&lt;p&gt;线性回归假设数据服从高斯分布，所以用一个平面(线性决策边界)可以把数据分开。但现实中数据并不都是完美的高斯分布，所以逻辑回归假设数据服从伯努利分布，通过逻辑函数(Sigmoid)作一层隐射，用一个曲面(非线性决策边界)去分开数据。逻辑回归的假设函数形式如下：&lt;br&gt;
&lt;img src=&quot;/images/lr/lr.png&quot; alt=&quot;note&quot; /&gt;
&lt;br&gt;
其实这就是个sigmoid函数，见下图：
&lt;img src=&quot;/images/lr/sigmoid.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;特征工程&lt;/h3&gt;

&lt;p&gt;因为LR比较朴实，表达能力较弱，所以做LR百分之80的工作其实是在做特征工程。毕竟只有找对了&lt;em&gt;因&lt;/em&gt;，才能到达正确的&lt;em&gt;果&lt;/em&gt;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;归一化:
归一化可以提高收敛速度，提高收敛的精度。具体参考&lt;a href=&quot;http://www.cnblogs.com/LBSer/p/4440590.html&quot;&gt;为什么一些机器学习模型需要对数据进行归一化？&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;离散化:
为什么要离散化，这里直接粘贴下网上找的：
&lt;img src=&quot;/images/lr/lisan.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;组合特征:
组合特征和特征离散化其实意义很类似，都是希望在更高阶的空间里找到划分曲面。通过组合低阶特征，提升模型的非线性表达。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特征相关性
训练的过程当中最好将高度相关的特征去掉：&lt;br&gt;
1) 去掉高度相关的特征会让模型的可解释性更好。&lt;br&gt;
2) 如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。&lt;br&gt;
3) 减少特征，自然会减少训练的时间。&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GBDT: 在处理特征离散化和组合特征的过程中有些问题需要人工经验来确定，比如&lt;br&gt;
1) 连续变量切分点如何选取？&lt;br&gt;
2) 离散化为多少份合理？&lt;br&gt;
3) 选择哪些特征交叉？多少阶交叉，二阶，三阶或更多？&lt;br&gt;
gbdt对于数据离散，会根据信息增益，客观的选取切分点和份数。而gbdt的叶子节点，其实就是特征组合的结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特征评估: 评估特征好坏的方法有ABtest，卡方检验，单特征AUC等。比较保险点办法就是在ab test系统去测试新加某个维度带来的ctr变化。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;损失函数&lt;/h3&gt;

&lt;p&gt;损失函数衡量的是实际值和预估值的差异。所以有很多函数可以来衡量这个距离：&lt;br&gt;
&lt;img src=&quot;/images/lr/cost.png&quot; alt=&quot;note&quot; /&gt;
LR一般用对数损失：cost(hθ(x),y)=∑i=1m−yilog(hθ(x))−(1−yi)log(1−hθ(x))。原因参考&lt;a href=&quot;https://www.jianshu.com/p/1bf35d61995f&quot;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h3&gt;模型训练&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;样本均衡: LR对样本均衡比较敏感，所以尽量还是保证正负样本1:1的比例。如果样本不均衡，可以参考&lt;a href=&quot;https://www.zhihu.com/question/56662976&quot;&gt;这里&lt;/a&gt;的方法。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型训练: lr的训练过程其实是一个无约束问题的求解过程：求最小的θ，使得损失函数最小。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法等。GD -&gt; BGD -&gt; SGD。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型评估:  // TODO&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;过拟合&amp;amp;欠拟合: // TODO&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;实现框架&lt;/h3&gt;

&lt;p&gt;SGD的分布式实现一直是业界比较关注的问题, 而问题的本质就是怎么减少进程间订单通讯开销、提高高维数据下的可扩展性。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;tree all reduce: spark的实现方式，两阶段树分裂算法。每次迭代会把所有参数广播到各个节点，然后分两阶段reduce到driver汇总更新。不管是速度，还是可扩展性，这种实现方式都比较差，所以spark的MLLib一直被人诟病。
&lt;img src=&quot;/images/lr/spark_sgd.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ps: 参数服务器，将参数和数据都分布式存储、训练、更新。比如&lt;a href=&quot;https://github.com/Angel-ML/angel&quot;&gt;tencent angel&lt;/a&gt;就是这种实现方式。
&lt;img src=&quot;/images/lr/angel_arc.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;collective communication: 集体通信，通过在进程间引入了同步点的概念达到进程间的同步更新。
&lt;img src=&quot;/images/lr/collective_comm.png&quot; alt=&quot;note&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;附录&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/walilk/article/details/51107380&quot;&gt;Coursera ML笔记 - 逻辑回归（Logistic Regression）&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://www.cnblogs.com/batys/p/3298816.html&quot;&gt;逻辑回归：使用SGD(Stochastic Gradient Descent)进行大规模机器学习&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://www.jianshu.com/p/1c2569c894ce&quot;&gt;LR模型的特征归一化和离散化&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/zh_cn/&quot;&gt;MPI 广播以及集体通信&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/06/24/lr/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/24/lr/</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>spark  - 那些坑</title>
        <description>&lt;h3&gt;JobProgressListener问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;Spark Application跑一段时间总会被yarn kill掉，报的错是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Container ... is running beyond physical memory limits. Current usage: xxx GB of xxx GB physical memory used; xx GB of xx GB virtual memory used. Killing container.&lt;/code&gt;&lt;/pre&gt;


&lt;!-- more --&gt;


&lt;p&gt;第一反应怀疑哪里有内存泄露，所以dump内存看了下，发现&lt;strong&gt;org.apache.spark.ui.jobs.JobProgressListener&lt;/strong&gt;几乎占掉了一半的内存。于是看代码，发现&lt;em&gt;stageIdToData&lt;/em&gt;和&lt;em&gt;jobIdToData&lt;/em&gt;两个变量保存了所有的job信息和stage信息。好在有&lt;em&gt;trimStagesIfNecessary&lt;/em&gt;和&lt;em&gt;trimJobsIfNecessary&lt;/em&gt;两个方法，当job数或者stage数超过配置的数目，JobProgressListener会每次remove掉10%的job或者stage。&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;spark.ui.retainedJobs
spark.ui.retainedStages&lt;/code&gt;&lt;/pre&gt;


&lt;h3&gt;Broadcast问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;使用broadcast的时候发现task耗费大量时间在&lt;em&gt;Task Deserialization Time&lt;/em&gt;上，且当广播的变量越大task反序列化的时间也越大。于是看到&lt;a href=&quot;http://dongguo.me/blog/2014/12/30/Spark-Usage-Share/&quot;&gt;这篇文章&lt;/a&gt;中提到的&lt;em&gt;正确地使用广播变量(broadcast variables)&lt;/em&gt;时才发现自己在真正产生action的操作前就读取broadcast的变量，导致每次广播都会在driver收集变量然后分发给各个executor。这是问题代码（java）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Function&lt;Row, Row&gt; mapFunc = new Function&lt;Row, Row&gt;() {
    private Map&lt;String, Double&gt; joinMap = broadcastVal.getValue();
    @Override
    public Row call(Row row) throws Exception {
        joinMap.get....
    }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;在action方法内部调用&lt;em&gt;getValue()&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Function&lt;Row, Row&gt; mapFunc = new Function&lt;Row, Row&gt;() {
    @Override
    public Row call(Row row) throws Exception {
        Map&lt;String, Double&gt; joinMap = broadcastVal.getValue();
        joinMap.get....
    }
}&lt;/code&gt;&lt;/pre&gt;


&lt;h3&gt;spark sql聚合函数问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;先说下背景。为了最大限度的减少每次查询cover的数据，我们的数据模型不仅按月分表，而且单月的数据都会做行转列的操作。简单点说就是把指标项转成31列，随着时间的发展去填充列而不是增加数据行数。这样做的好处就是每个月第一天和到最后一天的数据量都会差不多，而这样做的代价就是更加负责的sql和每次查询都要做列转行的反转。
举个栗子，查询20160301到20160325分年龄的收入，一般的sql是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT age, SUM(income) FROM  table_201603 WHERE date BETWEEN 20160301 AND 20160325 GROUP BY age&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;但是我们得这样处理&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT age, SUM(income_1)  AS inome_20160301, SUM(income_2)  AS inome_20160302... , SUM(income_25)  AS inome_20160325 FROM  table_201603 GROUP BY age&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;然后再flatten查询结果。
因为一般的查询结果集比较小，所以flatten结果集的代价也比较小。但是这样做在spark sql上碰到一个坑，当一条sql中&lt;strong&gt;SUM&lt;/strong&gt;的数量超过24个时候会严重影响sql的执行速度(5-10倍的性能差)。
//TODO 原因待查&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;目前的spark 1.6 还是存在这个问题，所以只好做和谐处理。当查询中&lt;strong&gt;SUM&lt;/strong&gt;超过24个，将sql拆成多条sql处理。&lt;/p&gt;

&lt;h3&gt;spark sql数据类型问题&lt;/h3&gt;

&lt;h5&gt;问题&lt;/h5&gt;

&lt;p&gt;数据中有个&lt;em&gt;曝光&lt;/em&gt;字段，生成模型的时候定义成了&lt;em&gt;int&lt;/em&gt;类型。sql的聚合函数&lt;em&gt;SUM&lt;/em&gt;会把结果转成&lt;em&gt;long&lt;/em&gt;类型。但是在测试的时候发现结果会变小。这个坑比较隐蔽，貌似只有当在单个task内的聚合值超过21亿的时候才会影响最终结果。&lt;/p&gt;

&lt;h5&gt;解决办法&lt;/h5&gt;

&lt;p&gt;数据源把&lt;em&gt;int&lt;/em&gt;改成&lt;em&gt;long&lt;/em&gt;类型就OK了&lt;/p&gt;
</description>
        <pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/05/17/spark-watchouts/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/05/17/spark-watchouts/</guid>
        
        
        <category>realtime</category>
        
      </item>
    
      <item>
        <title>spark core - Broadcast</title>
        <description>&lt;p&gt;最近刚完成一个spark sql项目：大数据实时查询引擎。所有的数据模型整合成大宽表以parquet(列存，高压缩比，完美整合spark sql)格式存在hdfs上。做成大宽表的好处是，一些费时的join操作在预处理阶段就做好了，查询时只会有简单的group by等操作。但是由于业务的特点，我们的历史数据会变(坑爹啊)，而且没有时限(一两年前的说变就变)。好在这部分变化的数据不大，可以经过&lt;strong&gt;spark broadcast&lt;/strong&gt;广播到各个executor进行rdd map join。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;说回这篇文章的主题-spark broadcast。spark目前有两种广播的方法：http和p2p。二者的区别是各个executor在第一次怎么拿到广播变量。一旦executor拿到广播变量后就会立即放到本地storage。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 把广播变量缓存到BlockManager，这样后续的任务就不需要再去重新fetch了
SparkEnv.get.blockManager.putSingle(blockId, value_, StorageLevel.MEMORY_AND_DISK, tellMaster = false)
&lt;/code&gt;&lt;/pre&gt;


&lt;h4&gt;HttpBroadcast&lt;/h4&gt;

&lt;p&gt;Http的广播方法比较简单，在driver写一个local file，起一个http server。所以executor在&lt;em&gt;getValue&lt;/em&gt;的时候其实就是通过url访问driver上的http server拿到广播变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//！代码都精简过的
/*
 * driver 写local file
 */
private def write(id: Long, value: Any) {
    // 拿到一个本地文件
    val file = getFile(id) 
    val fileOutputStream = new FileOutputStream(file)
    ...
   // 序列化
    val ser = SparkEnv.get.serializer.newInstance()
    val serOut = ser.serializeStream(out)
    ...
    serOut.writeObject(value)
 }
 /*
 * 起http server
 */
private def createServer(conf: SparkConf) {
  broadcastDir = Utils.createTempDir(Utils.getLocalDir(conf), &quot;broadcast&quot;)
  val broadcastPort = conf.getInt(&quot;spark.broadcast.port&quot;, 0)
  server =
    new HttpServer(conf, broadcastDir, securityManager, broadcastPort, &quot;HTTP broadcast server&quot;)
  server.start()
  serverUri = server.uri
}
 /*
 * 读广播变量
 */
private def read[T: ClassTag](id: Long): T = {
  // 组装请求
  val url = serverUri + &quot;/&quot; + BroadcastBlockId(id).name
  uc = new URL(url).openConnection()
  ...
   //请求广播变量
  val inputStream = uc.getInputStream
  val in = new BufferedInputStream(inputStream, bufferSize)
  ...
  // 反序列化
  val ser = SparkEnv.get.serializer.newInstance()
  val serIn = ser.deserializeStream(in)
}
&lt;/code&gt;&lt;/pre&gt;


&lt;h4&gt;TorrentBroadcast&lt;/h4&gt;

&lt;p&gt;Torrent采用spark storage模块广播数据。首先会按照特定大小（默认4M）对数据进行分块,  然后把各个piece写到storage。第一次读取变量也是从driver或者其他的executor拿到各个piece然后保存到本地。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//！代码都精简过的
/*
 * 数据写入storage
 */
private def writeBlocks(value: T): Int = {
  // driver上保存一份广播数据，防止重复存储
  SparkEnv.get.blockManager.putSingle(broadcastId, value, StorageLevel.MEMORY_AND_DISK,
    tellMaster = false)
   //对数据进行分块
  val blocks =
    TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec)
    // 各个块写到storage
  blocks.zipWithIndex.foreach { case (block, i) =&gt;
    SparkEnv.get.blockManager.putBytes(
      BroadcastBlockId(id, &quot;piece&quot; + i),
      block,
      StorageLevel.MEMORY_AND_DISK_SER,
      tellMaster = true)
  }
  blocks.length
}
 /*
 * 读取数据
 */
 private def readBlocks(): Array[ByteBuffer] = {
...
  //循环各个数据块
  for (pid &lt;- Random.shuffle(Seq.range(0, numBlocks))) {
    val pieceId = BroadcastBlockId(id, &quot;piece&quot; + pid)
    // 从本地取
    def getLocal: Option[ByteBuffer] = bm.getLocalBytes(pieceId)
    // 本地取不到则从远程取
    def getRemote: Option[ByteBuffer] = bm.getRemoteBytes(pieceId).map { block =&gt;
      // 取到保存到本地
      SparkEnv.get.blockManager.putBytes(
        pieceId,
        block,
        StorageLevel.MEMORY_AND_DISK_SER,
        tellMaster = true)
      block
    }
    val block: ByteBuffer = getLocal.orElse(getRemote).getOrElse(
      throw new SparkException(s&quot;Failed to get $pieceId of $broadcastId&quot;))
    blocks(pid) = block
  }
  blocks
}
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Mon, 16 May 2016 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2016/05/16/spark-broadcast/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/05/16/spark-broadcast/</guid>
        
        
        <category>realtime</category>
        
      </item>
    
      <item>
        <title>豆瓣读书推荐</title>
        <description>&lt;p&gt;之前还一直跟同事抱怨说做数据挖掘，没有数据挖个毛线啊。 于是naza很happy的把豆瓣读书的数据都爬下来了 ‘恩 你挖个够吧’。好吧， 一直想看看豆瓣读书的推荐到底是怎么做的。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;豆瓣读书的推荐主要有两个页面：
&lt;br /&gt;
一个是每本书的主页， 比如梁文道的&lt;a href=&quot;http://book.douban.com/subject/4031698/&quot;&gt;读者&lt;/a&gt;。豆瓣会有一个推荐的section: &lt;em&gt;喜欢读&quot;读者&quot;的人也喜欢...&lt;/em&gt;。 这句话似乎是说推荐是基于&lt;strong&gt;协同过滤&lt;/strong&gt;的。我很奇怪为什么豆瓣会用这样的标题，这里明显不会是基于协同的推荐， 因为我用不同的用户登录， 推荐的结果是一样的。从推荐的结果来也像是&lt;strong&gt;content based&lt;/strong&gt;， 更准确点是&lt;strong&gt;tag-based&lt;/strong&gt;。 因为推荐的大多还是梁文道的书，还有些书tag是&lt;em&gt;随笔&lt;/em&gt;、&lt;em&gt;文化&lt;/em&gt;什么的。
&lt;br /&gt;
另一个是&lt;a href=&quot;http://book.douban.com/recommended&quot;&gt;豆瓣猜&lt;/a&gt;。首先会列出你可能感兴趣的新书。之后是你可能感兴趣的tag和book。这里应该是混合推荐了。
&lt;br /&gt;
当然，这都是猜想。试试看吧。&lt;/p&gt;

&lt;h4&gt;Warm-up&lt;/h4&gt;

&lt;p&gt;先解释下数据模型，爬的数据有people，book，tag以及相互的对应关系。构建三个matrix就可以将三者联系起来了。如图:
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/douban_model.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
再说明下， 所有的算法都是based on mahout。&lt;/p&gt;

&lt;h4&gt;tag-based&lt;/h4&gt;

&lt;p&gt;给定一本书， 找到一本tag相似的书。可以把book-tag matrix作input，这样tag就相当于book的dimension, 可以用&lt;em&gt;PearsonCorrelationSimilarity&lt;/em&gt;， 即皮尔逊系数来衡量book之间的距离。这样找相似的书， 即找最近的邻居：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
NearestNUserNeighborhood neighbors = new NearestNUserNeighborhood(getHowMany(), similarity, model);
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;但是推荐的结果不尽人意。想到之前微博推荐的经验， 可以先对book根据book-tag matrix&lt;strong&gt;聚类&lt;/strong&gt;，再在每个cluster里找邻居，这样可以消除一些离群点的影响。看结果：
&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/rec_douban_tag.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
可以看到推荐的结果有&lt;a href=&quot;http://book.douban.com/subject/3644791/&quot;&gt;噪音太多&lt;/a&gt;、&lt;a href=&quot;http://book.douban.com/subject/1831760/&quot;&gt;弱水三千&lt;/a&gt;...&lt;/p&gt;

&lt;h4&gt;协同过滤&lt;/h4&gt;

&lt;p&gt;协同过滤， 说白了也是在people-book matrix里找邻居。不同的是user-based collaborative filtering是把book当成people的dimension。而item-based collaborative filtering则反过来。这里仍然用&lt;em&gt;PearsonCorrelationSimilarity&lt;/em&gt;来衡量相似度：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
UserSimilarity similarity = new PearsonCorrelationSimilarity(model);
NearestNUserNeighborhood neighbors = new NearestNUserNeighborhood(numNeighbor, similarity, model);
Recommender recommender = new GenericUserBasedRecommender(model, neighbors, similarity);
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;很遗憾没有爬到自己的账号。随便选个账号来测试，结果：
&lt;br /&gt;
&lt;br /&gt;
item-based
&lt;img src=&quot;/images/recommendation/rec_douban_item.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
user-based
&lt;img src=&quot;/images/recommendation/rec_douban_user.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
因为爬到的数据不多， 本来就稀疏的matrix交集更少。推荐出来的结果都是些可能看过一两本相同书的people给出的推荐。这也反映出协同过滤的cold start问题。不知道豆瓣是怎样解决的。
&lt;br /&gt;
&lt;br /&gt;
同上，我也试着对people根据people-tag matrix先聚类再做item-based collaborative filtering。推荐出来的比较像tag-based的结果。&lt;/p&gt;

&lt;h4&gt;new book recommendation&lt;/h4&gt;

&lt;p&gt;豆瓣还有个推荐新书的功能。根据爬到的数据，我能想到的方法就是计算people的历史tag跟新书tag之间distance，选距离最近的people。但是这时的距离应该是什么呢？ EuclideanDistance？试了下， 结果比较无语，因为推荐出来的都是些过分活跃的用户-那些所有tag都有比较大的weight的用户。
&lt;br /&gt;
&lt;br /&gt;
其实这也可以理解， content based必然会有热门问题，比如豆瓣的tag-based推荐，像&lt;em&gt;文学&lt;/em&gt;、&lt;em&gt;小说&lt;/em&gt;这样的tag就比较泛滥。 所以很多文学类小说的推荐常常会被几本热门的名著占据。 按理说豆瓣应该会对tag清理过，不明白为什么还是会有这样的问题。
&lt;br /&gt;
&lt;br /&gt;
显然要换个距离， 欧式距离会计算所有tag间的距离，是否可以让距离只考虑新书的tag在people上的weight。想到了对people向量做横向归一处理，这样就消除了热门tag对people的影响，也就能找到新书的tag占最大weigh的people。 看结果:
&lt;br /&gt;
&lt;img src=&quot;/images/recommendation/rec_douban_new.jpg&quot; alt=&quot;data model&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
这样就为&lt;a href=&quot;http://book.douban.com/subject/1041007/&quot;&gt;哈利·波特与魔法石&lt;/a&gt;找到了用户&lt;a href=&quot;http://book.douban.com/people/tuzicxy/collect&quot;&gt;tuzicxy&lt;/a&gt;、&lt;a href=&quot;http://book.douban.com/people/70621573/collect&quot;&gt;70621573&lt;/a&gt;...&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Aug 2013 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2013/08/16/recommendation-douban/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/08/16/recommendation-douban/</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>hadoop sort问题</title>
        <description>&lt;p&gt;项目组最近遇到一个hadoop(1.2)问题,因为map中产生的key太多,导致job花费了大量时间在map的shuffle上。准确来说时间应该都花在了IO上面，map产生的数据过多，使得shuffle阶段IO操作过多。 当然因为db设计的原因，map数据过大也是必然的事。  纵观Mapreduce的shuffle过程： sort、 IO 、merge， 似乎也只有sort还有优化的可能。&lt;/p&gt;

&lt;!-- more --&gt;


&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
这里先跑下题 Mapreduce本是一个聚合的过程, 但是因为后台db采用了星形的架构 使得map将数据发散到一个更广的空间中。 key多的点让人觉得不可理喻。 或许这种设计本身就不适合用Mapreduce来做ETL。
&lt;br /&gt;
&lt;br /&gt;
言归正传，于是项目组决定对Mapreduce的排序进行优化。 下面是优化的笔记：
&lt;br /&gt;
&lt;br /&gt;
Mapreduce的shuffle过程包括map阶段和reduce阶段。   在map阶段， 如图， 首先将输出写到缓存当中。 当缓存大小达到“阈值”时（默认的大小是缓存的80%），  变会进行'spill'： 即sort and combiner。  sort完才会把数据写进磁盘。  坑爹的是，  根据Mapreduce的设计(hadoop1.2)， sort是个无法避免的过程。
&lt;br /&gt;
&lt;img src=&quot;/images/hadoop/map_shuffle.jpg&quot; alt=&quot;map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在sort的过程中， 由于hadoop往缓存中写入的不是对象， 而是序列化以后的二进制数据。 所以在WritableComparator中， 会首先将这些二进制数据反序列化为对象， 然后再调用这些对象的compareTo方法进行比较：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
  try {
    buffer.reset(b1, s1, l1);                   // parse key1
    key1.readFields(buffer);
  
    buffer.reset(b2, s2, l2);                   // parse key2
    key2.readFields(buffer);
  
  } catch (IOException e) {
    throw new RuntimeException(e);
  }
  return compare(key1, key2);                   // compare them
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;所以说， 每次compare的过程都会有两个反序列化的过程。 当然还好hadoop留了一条后路， Job对象上有一个方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setSortComparatorClass(RawComparator c)
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;这样我们就能避免反序列化过程了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleComparator implements RawComparator{
  private Logger logger = Logger.getLogger(SampleComparator.class);

  @Override
  public int compare(Object o1, Object o2) {
    logger.error(&quot;SampleComparator does not support method compare(Object o1, Object o2)!&quot;);
    throw new RuntimeException(&quot;unsupported method!&quot;);
  }

  @Override
  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
    return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
  }
}
&lt;/code&gt;&lt;/pre&gt;


&lt;p&gt;当然， 进行这种修改的时候一定要确保是否序列化对排序结果没有影响。&lt;/p&gt;

&lt;h5&gt;附&lt;/h5&gt;

&lt;p&gt;顺便记一下最近看到的一个问题：
 为什么hadoop不适合做实时计算？ 个人的感觉主要是IO太多， 再加上启动时间、 网络传输等因素 ，就决定了hadoop在实时这个领域难有作为。
&lt;br /&gt;storm就不一样，数据都是在内存中， task一直在监听数据， 除非kill掉(下次系统看下storm)&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jul 2013 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2013/07/14/hadoop-sort/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/07/14/hadoop-sort/</guid>
        
        
        <category>大数据</category>
        
      </item>
    
  </channel>
</rss>
