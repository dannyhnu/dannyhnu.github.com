---
layout: post
title: "ALGO笔记 - LR"
description: 广告CTR算法 LR
categories : algorithm
---
LR一般用来解决二分类问题，表达能力比较弱，可以简单的这样理解：LR是一个拟合因和果的算法，它认为任何结果y必定和一系列因x存在某些关系w。虽然比较朴实，但LR真的是一个万金油的算法，我们在很多工作中(CTR预估、游戏推荐、系统运维...)都用到了。因为LR：<br>
<!-- more -->
1. 简单、有效(数据量大到一定程度，这些模型的效果其实差别并不大), 可解释性好。
2. 简单易部署，分布式训练算法比较成熟(在大数据场景下，训练速度快、消耗资源少)，适合online learning。
3. 稳定。

####基本假设
线性回归假设数据服从高斯分布，所以用一个平面(线性决策边界)可以把数据分开。但现实中数据并不都是完美的高斯分布，所以逻辑回归假设数据服从伯努利分布，通过逻辑函数(Sigmoid)作一层隐射，用一个曲面(非线性决策边界)去分开数据。逻辑回归的假设函数形式如下：<br>
![note](/images/lr/lr.png)
<br>
其实这就是个sigmoid函数，见下图：
![note](/images/lr/sigmoid.png)

####特征工程
因为LR比较朴实，表达能力较弱，所以做LR百分之80的工作其实是在做特征工程。毕竟只有找对了*因*，才能到达正确的*果*。

* 归一化
归一化可以提高收敛速度，提高收敛的精度。具体参考[为什么一些机器学习模型需要对数据进行归一化？](http://www.cnblogs.com/LBSer/p/4440590.html);

* 离散化
为什么要离散化，这里直接粘贴下网上找的：
![note](/images/lr/lisan.png)

* 组合特征
组合特征和特征离散化其实意义很类似，都是希望在更高阶的空间里找到划分曲面。通过组合低阶特征，提升模型的非线性表达。

* 特征相关性
训练的过程当中最好将高度相关的特征去掉：<br>
1. 去掉高度相关的特征会让模型的可解释性更好
2. 如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。
3. 减少特征，自然会减少训练的时间。

* GBDT

* 特征评估

####损失函数

####模型训练

* 样本选择

* GD

* SGD

####模型评估

* 精准度

* 召回率

* AUC

* 过拟合&欠拟合

####实现框架

* SPARK ML

* angel

### 附录
[Coursera ML笔记 - 逻辑回归（Logistic Regression）](https://blog.csdn.net/walilk/article/details/51107380)<br>
[逻辑回归：使用SGD(Stochastic Gradient Descent)进行大规模机器学习](https://www.cnblogs.com/batys/p/3298816.html)<br>
[LR模型的特征归一化和离散化](https://www.jianshu.com/p/1c2569c894ce)<br>