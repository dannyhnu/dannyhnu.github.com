---
layout: post
title: "machine learning 笔记"
description: machine learning的一些笔记
categories : algorithm
---
接触**machine learning**一年多了 也攒了不少笔记 话说期间各种算法都有接触过 分类 聚类  推荐 降维...  或许是research的不够深入的原因吧  我发现其实大多数算法都可以用一句话概况
<!-- more -->
<br />

####分类
#####Bayesian
最有可能属于哪一类
<br />
![bayesian](/images/note/ml/bayesian.jpg)
#####SVM
找出一个能划分两个类的超平面
<br />
![SVM](/images/note/ml/svm.jpg)
#####Decision tree
每个枝上选一个维度去划分节点上的所有样本
<br />
![decision tree](/images/note/ml/dt.jpg)
#####ANN
函数黑盒  这个似乎有点高深  现在也不知道能不能再推出来
<br />
![ANN](/images/note/ml/ann_1.jpg)
<br />
![ANN](/images/note/ml/ann_2.jpg)


####距离
各种距离  最常用的是欧式和余弦  可能根据应用场景的不同有些差异  比如 点 p(5, 5), p(1, 1) 和 p(0, 0)之间的距离。 这种差异可以通过归一化来handle
<br />
![distance](/images/note/ml/dis.jpg)

####聚类
#####kmeans
最基础的聚类算法  *选中心点*  *不停的迭代*
<br />
![kmeans](/images/note/ml/kmeans.jpg)
#####Spectral
谱聚类
<br />
![spectral](/images/note/ml/spectral_1.jpg)
<br />
![spectral](/images/note/ml/spectral_2.jpg)

####降维
LDA是监督学习  将不同的样本映射到*sub space*上  使得不同label之间的方差最大化 但是同一label内的样本尽量靠近各自的中心
<br />
PCA是无监督学习  也是将所有样本映射到*sub space*上 不同的是降维后使得所有样本的方程最大化
<br />
![pca](/images/note/ml/pca_1.jpg)
<br />
![pca](/images/note/ml/pca_2.jpg)

####推荐
基于内容的  基于协同的  还有关联规则的
#####关联规则
*支持度*  *置信度* *频繁项集*
<br />
![asso](/images/note/ml/asso.jpg)
#####条件概率
阿里技术博客上看到的 很简单的based on**条件概率**的 推荐
<br />
![ali](/images/note/ml/rec_ali.jpg)

####Note
不记得哪里看到的一条机器学习的三维
<br />
![note](/images/note/ml/ml_3.jpg)